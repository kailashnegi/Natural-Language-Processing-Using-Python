{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Course - Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is the the leading library of NLP and it has become one of the most popular frameworks. It has very rich [documentation](https://spacy.io/usage) as well. It actually has an interactive four lesson [tutorial](https://course.spacy.io/en/) where you can learn all about spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation instructions are already there in the documenation. However if you work on jupyter notebook in conda environment like me, please run the below command in Anaconda Prompt (Run as administrator) to start your spaCy journey.\n",
    "\n",
    "****Step1 : Install spaCy****<br/>\n",
    "conda install -c conda-forge spacy\n",
    "\n",
    "****Step2 : Install Language Model****<br/>Choose one between these two lanugage models<br/><br/>\n",
    "python -m spacy download en  #default English model (~50MB)<br/><br/>\n",
    "python -m spacy download en_core_web_md # larger English model (1GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is used primarily for following purposes.\n",
    "- Basic text processing and pattern matching\n",
    "- Building machine learning models with text\n",
    "- Representing text with word embeddings that numerically capture the meaning of words or documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy relies on ****models**** that are language specific and come in different sizes. Here is how we load the English language spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is loaded we can process text, see below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this doc object created, we can do a lot with it...like..\n",
    "- Tokenizing\n",
    "- Lemmatizing\n",
    "- Removing stop words\n",
    "- Pattern Matching\n",
    "<br/>etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Tokenizing****\n",
    "<br/>Its a process that returns a document object containing tokens. A token is nothing but a unit of text in the document such as individual words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Lemmatizing****\n",
    "<br/>Its a process of taking a word back to its original or base form. For example lemmatizing the word \"running\" will give us \"run\". We can perform this on Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tea\n",
      "be\n",
      "healthy\n",
      "and\n",
      "calm\n",
      ",\n",
      "do\n",
      "not\n",
      "-PRON-\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Removing Stop Words****\n",
    "<br/>Stop words removal is a process of removing words that occur very frequently in the document and don't add much information. We can check if a word is stop word or not. Words such as \"the\", \"is\", \"but\", \"and\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize the three steps like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalm\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "you\t\t-PRON-\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern Matching\n",
    "Pattern matching is the process of matching tokens or phrases within a document. Compared to using regular expressions on raw text, spaCy’s rule-based matcher engines and components not only let you find the words and phrases you’re looking for – they also give you access to the tokens within the document and their relationships. This means you can easily access and analyze the surrounding tokens, merge spans into single tokens or add entries to the named entities in doc.ents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match individual tokens, you create a **Matcher**. When you want to match a list of terms, it's easier and more efficient to use **PhraseMatcher**. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the <b>PhraseMatcher</b> itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier which we have named <b>nlp</b>. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitive matching. The thing to keep in mind is the object <b>matcher</b> is actually <b>PhraseMatcher</b> for future reference.\n",
    "\n",
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the model that you are using (in our case <b>nlp</b> model).\n",
    "\n",
    "Then we will add a match-rule to the phrase-matcher. A match-rule consists of: an ID\n",
    "key, an on_match callback, and one or more patterns. In our case the pattern is the list of Smartphone models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you create a document from the text in which we want to search and use the phrase matcher to find where the terms occur in the text. In our case we will search the Smartphone models in the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches that we get are tuple of the match id and the positions of the start and end of the phrase. For example in the current case the first match id is \"3766102292120407359\", start position is \"17\" and end position is \"19\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can search for the exact string using the match id and start and end position is the text block like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone 11\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
